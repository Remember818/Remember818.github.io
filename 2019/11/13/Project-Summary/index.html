
<!DOCTYPE html>
<html lang="en" class="loading">
<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>Project-Summary - Kevin&#39;s blog</title>
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="google" content="notranslate" />
    <meta name="keywords" content="TriDiamond Obsidian,"> 
    <meta name="description" content="IntroductionWith the Internet becoming more accessible to people nowadays, online users leave treme,"> 
    <meta name="author" content="Kevin"> 
    
    <link rel="icon" href="/img/favicon.png"> 
    <link href="https://fonts.loli.net/css?family=Roboto+Mono|Rubik&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="//at.alicdn.com/t/font_1429596_nzgqgvnmkjb.css">
    <link rel="stylesheet" href="//cdn.bootcss.com/animate.css/3.7.2/animate.min.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/css/share.min.css">
    <link rel="stylesheet" href="//cdn.bootcss.com/codemirror/5.48.4/codemirror.min.css">
    <link rel="stylesheet" href="//cdn.bootcss.com/codemirror/5.48.4/theme/dracula.css">
    <link rel="stylesheet" href="/css/obsidian.css">
    <link rel="stylesheet" href="/css/ball-atom.min.css">
</head>

<body class="loading">
    <div class="loader">
        <div class="la-ball-atom la-2x">
            <div></div>
            <div></div>
            <div></div>
            <div></div>
        </div>
    </div>
    <span id="config-title" style="display:none">Kevin&#39;s blog</span>
    <div id="loader"></div>
    <div id="single">
    <div class="scrollbar gradient-bg-rev"></div>
<div id="top" style="display: block;">
    <div class="bar" style="width: 0;"></div>
    <div class="navigation animated fadeIn fast delay-1s">
        <img id="home-icon" class="icon-home" src="/img/favicon.png" alt="" data-url="http://yoursite.com">
        <div id="play-icon" title="Play/Pause" class="iconfont icon-play"></div>
        <h3 class="subtitle">Project-Summary</h3>
        <div class="social">
            <!--        <div class="like-icon">-->
            <!--            <a href="javascript:;" target="_blank" rel="noopener" class="likeThis active"><span class="icon-like"></span><span class="count">76</span></a>-->
            <!--        </div>-->
            <div>
                <div class="share">
                    
                </div>
            </div>
        </div>
    </div>
</div>

    <div class="section">
        <div class= article-header-wrapper>
    <div class="article-header">
        <div class="article-cover animated fadeIn" style="
            animation-delay: 600ms;
            animation-duration: 1.2s;
            background-image: 
                radial-gradient(ellipse closest-side, rgba(0, 0, 0, 0.65), #100e17),
                url(/img/cover.jpg);">
        </div>
        <div class="else">
            <p class="animated fadeInDown">
                
                    <a href="javascript:;" target="_blank" rel="noopener"><b>「 </b>Article<b> 」</b></a>
                
                November 13, 2019
            </p>
            <h3 class="post-title animated fadeInDown"><a href="/2019/11/13/Project-Summary/" title="Project-Summary" class="">Project-Summary</a></h3>
            
                <p class="post-count animated fadeInDown">
                    
                        <span>
                        <b class="iconfont icon-text2"></b> <i>Words count</i>
                        15k
                    </span>
                    
                    
                        <span>
                        <b class="iconfont icon-timer__s"></b> <i>Reading time</i>
                        14 mins.
                    </span>
                    
                    
                    
                </p>
            
            
        </div>
    </div>
</div>

<div class="screen-gradient-after">
    <div class="screen-gradient-content">
        <div class="screen-gradient-content-inside">
            <div class="bold-underline-links screen-gradient-sponsor">
                <p>
                    <span class="animated fadeIn delay-1s"></span>
                </p>
            </div>
        </div>
    </div>
</div>

<div class="article">
    <div class='main'>
        <div class="content markdown animated fadeIn slower">
            <h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>With the Internet becoming more accessible to people nowadays, online users leave tremendous opinions and comments online. Those online message gives the netizen’s attitude in terms of certain topics or events. For example, the comments on the e-commercial website are classified according to their attitudes: positive, negative or neutral, to give a quick assessment of a product. However, the extreme large size of the online comments or opinion make it nearly infeasible to manually classify them. In the last 20 years, with well-study in the field of natural language processing (NLP)[1] and machine learning, various models were applied on the problem, called short text sentimental classification, and reaches pretty good result. In this report we will present the promising results yield by recurrent neural network with LSTM architecture.</p>
<p>One classic model used in classification problem is SVM[2]. Generally, in the field of machine learning, we first split the given data set into 2 subsets: training and testing data set. Secondly, we trained the model with the training data set which is the ground truth such that a hyperplane[3] or more can be fit to classify the data set into 2 or more groups. However, one principle is needed to guide us to find the optimal hyperplane, and loss function[4] could be that guidance. The loss function used in SVM is hinge loss which are designed to lead to a “maximum-margin” hyperplane to separate two subsets. Nevertheless, SVM does not perform well with large scale data set or noisy data set.</p>
<p>Indeed, based on our experiment results, SVM is not suitable for this large scale data set. It is time-consuming. Because of the drawbacks of the conventional machine learning algorithm or model, people started to seek for a better method of classification, and soon they put their eyes on the neural network. Neural network not only can deal with large scale data set, but also it can learn multiple patterns or features from the data set.</p>
<p>Recently, deep learning shows great achievement on text analysis[5]. Also, the long short-term memory(LSTM)[6] is becoming dominant when we have continued data such as a paragraph with continued sentences. Because of this architecture will utilize the fact that the order of the sentences is meaningful, other than conventional neural network, LSTM gets the ability to learn the context information.</p>
<h3 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h3><p><strong>A. SVM</strong></p>
<p>SVM is a simple linear classifier. It learns to classify new data by training on known data and develop a hyperplane. In order to optimize this hyperplane and simplify the process, we need to tune hour parameters: kernel[7], regularizer[7], gamma[7], and margin[7]. </p>
<p>The diagrams below illustrate what kernel is. It is a process of computing a complex, high-dimensional problem in a simple way, like converting sentence into vector. For instance, using bag-of words method, we can express a sentence in a series of numbers.</p>
<p><img src="/2019/11/13/Project-Summary/1.png" alt="Converting Text to points"></p>
<p>Constructing a hyperplane for the classifier, we need a regularizer. The left diagram shows a hyperplane with low regularization value, while the right diagram shows a hyperplane with higher regularization. The regularization value determines how conforming the hyperplane is. </p>
<p><img src="/2019/11/13/Project-Summary/2.png" alt="Low regularization value &amp; High regularization value"></p>
<p>Gamma determines which points the model will consider and how the hyperplane will be influenced. Low gamma will make the model consider far away points, whilst high gamma will make the model consider nearby points only.</p>
<p><img src="/2019/11/13/Project-Summary/3.png" alt="Low gamma &amp; High gamma"></p>
<p>A margin determines how close is the hyperplane to different groups. A good margin indicates that the hyperplane will locate at the middle between two groups and will not incline to any of them, while a bad margin means that the hyperplane incline to one of the classes.</p>
<p><img src="/2019/11/13/Project-Summary/4.png" alt="Good margin &amp; Bad margin"></p>
<p><strong>B. Neural Network</strong></p>
<p><img src="/2019/11/13/Project-Summary/5.png" alt="Illustration of Neural Network"></p>
<p>Deep learning is a branch of machine learning. Similarly, its aim is to learn a particular process and develop the most suitable model for it. It tries to simulate the mechanism of human brain. Different functions are recognized as different neurons, and have their own preferences, called weights[8], and their own stereotypes, called bias[9]. There are several important compositions of the neural network.</p>
<p>First, the input layer is where the data is fed. After computing with in-built function, it will then pass the data as outputs to the next layer. The next layer receives the outputs from previous layer as its inputs and keeps computing it. This process repeats over and over again, and the information is transferred in the network. Finally, it will come out as a desired output.</p>
<p>During the transfer of information, except the functions in each neuron, the weights will also affect the inputs. It indicates the importance of the inputs. It is the coefficients of the functions. During the training, the weights are constantly optimized.</p>
<p>In each layer, there is an independent constant called bias. It is an addition to the result of the function. With its value, the results are shifted. Also, it ensures that the network can still work even when the input is zero. It is like the the y-intercept of slope-intercept form, which is constant and ensure that y has a value even when the x is equal to zero.</p>
<p><strong>C. LSTM</strong></p>
<p><img src="/2019/11/13/Project-Summary/6.png" alt="Conventional RNN"></p>
<p>LSTM is a branch of RNN[10]. The diagram above demonstrates what it is. Comparing with feedforward network, the RNN not only produce an output, but also recurrent the output in its network, creating a memory. Thus, it is widely used to deal with sequence of information such as pictures, handwriting, etc. However, RNN cannot correct the error in the output immediately and the error will also be sent back into the loop. As a result, the error will be multiplied, ultimately resulting in extreme output. This is vanishing gradient problem[11].</p>
<p><img src="/2019/11/13/Project-Summary/7.png" alt="RNN with LSTM architecture"></p>
<p>To solve this problem, LSTM is developed. Comparing with traditional RNN, LSTM is added with three more independent gates: input gate[12], forget gate[12], and output gate[12], which can control the recurrent flow like a door. The diagram above illustrates how they work. The input gate decides whether to take new input or circulate the old output; the forget gate determine whether erase or keep the information in the circulation; the output gate judge whether produce an output or send it back into the loop. The gates work together, block or pass input, and maintain a constant error, in order to avoid vanishing gradient problem.</p>
<h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>From Figure Eight, I obtained two data sets. One was about the sentiments towards the weather of Twitter users, and the other one was about the sentiments towards various airlines. I first tokenized the texts, and then filtered all neutral sentiments of both data sets.<br>Overall, the “Weather” data set included less data. Excluded the neutral sentiments, about 1000 texts are within. The “Airline” data set included more data. Considering only the positive and negative sentiments, around 11000 texts are within.<br>I lastly divided both data set into training set and testing set at a ratio of 7 to 3.</p>
<p><img src="/2019/11/13/Project-Summary/8.png" alt="Size of training and testing set"></p>
<h3 id="Experimental-Setup-and-Results"><a href="#Experimental-Setup-and-Results" class="headerlink" title="Experimental Setup and Results"></a>Experimental Setup and Results</h3><p><strong>A. Model Setting</strong></p>
<p>As it was impossible to compare the effectiveness of all the models in history, I planed to compare the efficiency of traditional and up-to-date methods of classification: SVM classifier and LSTM neural network. In addition, other factors affecting the accuracy of the performance, such as data size, activation, number of layers, will be tuned, in order to find the optimum setting for the performance of the classifier with different data size.</p>
<p>Overall, the neural network was composed of one input layer, one embedding layer, one LSTM layer, two dense layers, two activation layers and one dropout layer.</p>
<p><strong>B. Regularization</strong></p>
<p>For SVM classifier, I just trained and tested it with different kernel with different sizes of data sets. Whereas for LSTM neural network, I varied the dropout rate between 0 and 0.5, varied the embedding layer between 100 and 10000, and varied the LSTM layer between 16 and 64.</p>
<p><strong>C. Training and Validation</strong></p>
<p>Before training I implemented Tokenizer, which ignores the grammar of words, and implements bag-of-word, to convert the text into arrays. Then, I trained both the classifiers with the converted texts.</p>
<p>For SVM, two kernels were tested: rbf kernel which was a non-linear kernel, and linear kernel. Time and accuracy were recorded.</p>
<p>For LSTM neural network, I noticed that it performed better with the dropout rate 0.2 and LSTM layer value of 32. Any increase or decrease will reduce the accuracy. Considering the embedding, I found that the more the layers, the higher the accuracy. However, the time required and load on the computer would increase dramatically. Thus, I believed that 1000 embedding layers would be enough.</p>
<p><strong>D. Result and Discussion</strong></p>
<p>I performed the training and testing for several times in order to compare the effectiveness of traditional method and advanced method. As shown in the table above, LSTM neural network performed significantly better than SVM did.</p>
<p>After tuning, with the best setting for the neural network, the accuracy of LSTM reached 89% in testing. However, in the “Airline” data set, as the amount of negative comment was significantly greater than that of positive comments, the accuracy for judging positive comment was only 75% whilst the accuracy for judging negative comments was 94%.</p>
<p>For SVM, it was obvious that its performance was worse than that of LSTM. With rbf kernel, the accuracy in classifying “Airline” data set was 80%, while the accuracy in classifying “Weather” data set was only 50%. Similarly, with linear kernel, the accuracy for classifying both data sets were almost the same. Nevertheless, the computing times under linear kernel were much higher than other models or kernels. In training with “Airline” data set, the training time was more than 2 hours, whereas in training with “Weather” data set, a smaller data set, the training time reached 18 minutes.</p>
<p>Therefore, it can be seen that LSTM performed better than traditional classifier, as it computed faster with higher accuracy.</p>
<p><img src="/2019/11/13/Project-Summary/9.png" alt="Result of the experiment"></p>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>In this report, I examined and compared the performances of new and old classifiers, based on data set about Twitter users. I found a better way of tuning the neural network and achieved a better result. It was obvious that advanced LSTM performed better and more efficiently than SVM linear classifier.<br>This definitely showed the trend towards solving data problem with neural network and machine learning.</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1]<a href="https://books.google.com/books?hl=en&lr=&id=KGIbfiiP1i4C&oi=fnd&pg=PR5&dq=natural+language+processing&ots=Y3FiD0OAF8&sig=Ijjfeuc5feUaP0YIRp6FBKaSdrM#v=onepage&q=natural%20language%20processing&f=false" target="_blank" rel="noopener">natural language processing(Link is too long to show on the screen)</a></p>
<p>[2]SVM</p>
<p><a href="https://medium.com/machine-learning-101/chapter-2-svm-support-vector-machine-theory-f0812effc72" target="_blank" rel="noopener">https://medium.com/machine-learning-101/chapter-2-svm-support-vector-machine-theory-f0812effc72</a></p>
<p>[3]Hyperplane</p>
<p><a href="https://towardsdatascience.com/https-medium-com-pupalerushikesh-svm-f4b42800e989" target="_blank" rel="noopener">https://towardsdatascience.com/https-medium-com-pupalerushikesh-svm-f4b42800e989</a></p>
<p>[4]Loss function</p>
<p><a href="https://en.wikipedia.org/wiki/Loss_function" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Loss_function</a></p>
<p>[5]<a href="https://ieeexplore.ieee.org/abstract/document/7550882" target="_blank" rel="noopener">https://ieeexplore.ieee.org/abstract/document/7550882</a></p>
<p>[6]Long short-term memory(LSTM)</p>
<p><a href="https://en.wikipedia.org/wiki/Long_short-term_memory" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Long_short-term_memory</a></p>
<p>[7]kernel, regularizer, gamma, and margin</p>
<p><a href="https://medium.com/machine-learning-101/chapter-2-svm-support-vector-machine-theory-f0812effc72" target="_blank" rel="noopener">https://medium.com/machine-learning-101/chapter-2-svm-support-vector-machine-theory-f0812effc72</a></p>
<p>[8]Weights</p>
<p><a href="https://datascience.stackexchange.com/questions/19099/what-is-weight-and-bias-in-deep-learning" target="_blank" rel="noopener">https://datascience.stackexchange.com/questions/19099/what-is-weight-and-bias-in-deep-learning</a></p>
<p>[9]Bias</p>
<p><a href="https://datascience.stackexchange.com/questions/19099/what-is-weight-and-bias-in-deep-learning" target="_blank" rel="noopener">https://datascience.stackexchange.com/questions/19099/what-is-weight-and-bias-in-deep-learning</a></p>
<p>[10]Recurrent neural network(RNN)</p>
<p><a href="https://en.wikipedia.org/wiki/Recurrent_neural_network" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Recurrent_neural_network</a></p>
<p>[11]Vanishing gradient</p>
<p><a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Vanishing_gradient_problem</a></p>
<p>[12]Input gate, output gate, and forgot gate</p>
<p><a href="https://en.wikipedia.org/wiki/Long_short-term_memory" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Long_short-term_memory</a></p>

            <!--[if lt IE 9]><script>document.createElement('audio');</script><![endif]-->
            <audio id="audio" loop="1" preload="auto" controls="controls" data-autoplay="false">
                <source type="audio/mpeg" src="">
            </audio>
            
                <ul id="audio-list" style="display:none">
                    
                        
                            <li title='0' data-url='/statics/chengdu.mp3'></li>
                        
                    
                </ul>
            
            
            
        </div>
        <div class="sidebar">
            <div class="box animated fadeInRight">
                <div class="subbox">
                    <img src="https://res.cloudinary.com/tridiamond/image/upload/v1573019751/TriDiamond_logo_ui_xeublz.jpg" height=300 width=300></img>
                    <p>Kevin</p>
                    <span>Develop for fun</span>
                    <dl>
                        <dd><a href="https://github.com/Remember818/Remember818.github.io" target="_blank"><span class=" iconfont icon-github"></span></a></dd>
                    </dl>
                </div>
                <ul>
                    <li><a href="/">3 <p>Articles</p></a></li>
                    <li><a href="/categories">0 <p>Categories</p></a></li>
                    <li><a href="/tags">0 <p>Tags</p></a></li>
                </ul>
            </div>
            
                
                
                    <div class="box sticky animated fadeInRight faster">
                        <div id="toc" class="subbox">
                            <h4>Contents</h4>
                            <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Introduction"><span class="toc-number">1.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Architecture"><span class="toc-number">2.</span> <span class="toc-text">Architecture</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Dataset"><span class="toc-number">3.</span> <span class="toc-text">Dataset</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Experimental-Setup-and-Results"><span class="toc-number">4.</span> <span class="toc-text">Experimental Setup and Results</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Conclusion"><span class="toc-number">5.</span> <span class="toc-text">Conclusion</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Reference"><span class="toc-number">6.</span> <span class="toc-text">Reference</span></a></li></ol>
                        </div>
                    </div>
                
            
        </div>
    </div>
</div>


    </div>
</div>
    <div id="back-to-top" class="animated fadeIn faster">
        <div class="flow"></div>
        <span class="percentage animated fadeIn faster">0%</span>
        <span class="iconfont icon-top02 animated fadeIn faster"></span>
    </div>
</body>
<footer>
    <p class="copyright" id="copyright">
        &copy; 2019
        <span class="gradient-text">
            Kevin
        </span>.
        Powered by <a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a>
        Theme
        <span class="gradient-text">
            <a href="https://github.com/TriDiamond/hexo-theme-obsidian" title="Obsidian" target="_blank" rel="noopener">Obsidian</a>
        </span>
        <small><a href="https://github.com/TriDiamond/hexo-theme-obsidian/blob/master/CHANGELOG.md" title="v1.4.2" target="_blank" rel="noopener">v1.4.2</a></small>
    </p>
</footer>

<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
<script src="/js/plugin.js"></script>
<script src="/js/obsidian.js"></script>
<script src="/js/jquery.truncate.js"></script>
<script src="/js/search.js"></script>
<script src="//cdn.bootcss.com/typed.js/2.0.10/typed.min.js"></script>
<script src="//cdn.bootcss.com/blueimp-md5/2.12.0/js/md5.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/js/social-share.min.js"></script>

<script src="https://cdn.bootcss.com/codemirror/5.48.4/codemirror.min.js"></script>

    <script src="//cdn.bootcss.com/codemirror/5.48.4/mode/javascript/javascript.min.js"></script>

    <script src="//cdn.bootcss.com/codemirror/5.48.4/mode/css/css.min.js"></script>

    <script src="//cdn.bootcss.com/codemirror/5.48.4/mode/xml/xml.min.js"></script>

    <script src="//cdn.bootcss.com/codemirror/5.48.4/mode/htmlmixed/htmlmixed.min.js"></script>

    <script src="//cdn.bootcss.com/codemirror/5.48.4/mode/clike/clike.min.js"></script>

    <script src="//cdn.bootcss.com/codemirror/5.48.4/mode/php/php.min.js"></script>

    <script src="//cdn.bootcss.com/codemirror/5.48.4/mode/shell/shell.min.js"></script>

    <script src="//cdn.bootcss.com/codemirror/5.48.4/mode/python/python.min.js"></script>




<link rel="stylesheet" href="//cdn.bootcss.com/photoswipe/4.1.3/photoswipe.min.css">
<link rel="stylesheet" href="//cdn.bootcss.com/photoswipe/4.1.3/default-skin/default-skin.min.css">
<script src="//cdn.bootcss.com/photoswipe/4.1.3/photoswipe.min.js"></script>
<script src="//cdn.bootcss.com/photoswipe/4.1.3/photoswipe-ui-default.min.js"></script>

<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">
    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>
    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">
        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>
        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">
            <div class="pswp__top-bar">
                <!--  Controls are self-explanatory. Order can be changed. -->
                <div class="pswp__counter"></div>
                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
                <button class="pswp__button pswp__button--share" title="Share"></button>
                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>
            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>
            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>
            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>
            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>
        </div>
    </div>
</div>



    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="//www.googletagmanager.com/gtag/js?id=UA-149874671-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-149874671-1');
    </script>





<script>
    function initialTyped () {
        var typedTextEl = $('.typed-text');
        if (typedTextEl && typedTextEl.length > 0) {
            var typed = new Typed('.typed-text', {
                strings: ["Develop for fun", ""],
                typeSpeed: 90,
                loop: true,
                loopCount: Infinity,
                backSpeed: 20,
            });
        }
    }

    if ($('.article-header') && $('.article-header').length) {
        $(document).ready(function () {
            initialTyped();
        });
    }
</script>



</html>
